
# ğŸ“˜ Fake News Detection 

## ğŸ¯ Objective
This assignment aims to develop a **Semantic Classification model** to distinguish between true and fake news articles. Using the **Word2Vec** method, we capture semantic relations and apply supervised learning models to classify news based on meaning rather than syntax.

## ğŸ¢ Business Objective
The rampant spread of fake news has made it crucial to deploy intelligent systems that can automate the detection of misinformation. This project demonstrates how semantic classification can help news platforms and users verify content effectively.

## ğŸ—‚ï¸ Dataset Description
- ğŸ”¹ True.csv â€“ 21,417 records of true news articles.
- ğŸ”¹ Fake.csv â€“ 23,502 records of fake news articles.

Each dataset contains:
- ğŸ“Œ `title`: Title of the news article
- ğŸ“Œ `text`: Main content of the news article
- ğŸ“Œ `date`: Date of publication

## ğŸ“Š Observed Patterns
- âœ… **True News**: Formal language, factual nouns, domain-specific terminology, structured writing.
- âš ï¸ **Fake News**: Emotionally charged language, repetitive phrases, informal structure, frequent misleading n-grams.
- ğŸ“ˆ Word clouds and frequency analysis showed distinct linguistic styles across classes.

## ğŸ”§ Methodology

### ğŸ§¹ Preprocessing
- ğŸ”¹ Text cleaning
- ğŸ”¹ Lemmatization
- ğŸ”¹ POS filtering (focused on nouns)

### ğŸ§  Feature Engineering
- ğŸ”¹ Used pre-trained Word2Vec embeddings to convert text into semantic vectors.

### ğŸ§ª Models Used
- ğŸ”¹ Logistic Regression
- ğŸ”¹ Decision Tree
- ğŸ”¹ Random Forest

## ğŸ“ Model Evaluation
- ğŸ“Š **Validation Accuracy**: 0.8600
- ğŸ“Š **Precision**: 0.8590
- ğŸ“Š **Recall**: 0.8701
- ğŸ“Š **F1 Score**: 0.8645

### ğŸ“„ Classification Report

| Label      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0 (Fake)   | 0.86      | 0.85   | 0.86     | 73      |
| 1 (True)   | 0.86      | 0.87   | 0.86     | 77      |
| **Overall**| **0.86**  | **0.86**| **0.86** | **150** |

- **Macro Avg**: Precision = 0.86, Recall = 0.86, F1 = 0.86  
- **Weighted Avg**: Precision = 0.86, Recall = 0.86, F1 = 0.86

## ğŸ” Conclusion
- ğŸ”¹ Semantic modeling improves classification robustness.
- ğŸ”¹ Word2Vec embeddings enrich feature quality without needing deep learning.
- ğŸ”¹ Random Forest performs best for this type of text classification.
- ğŸ”¹ The model supports scalable and practical deployment for misinformation detection.

## ğŸ”® Future Work
- ğŸ”¹ Fine-tune Word2Vec on domain-specific news data.
- ğŸ”¹ Integrate transformer-based embeddings (e.g., BERT) for context-aware classification.

---

ğŸ–‹ï¸ **Author**: Sravana Sanka & Shriyan
